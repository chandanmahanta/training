Spark Practicals
----------------

1. To Login
-----------
$ spark-shell

OR

spark-shell --driver-memory 2G

OR
spark-shell --conf spark.ui.port=4050 --driver-memory 2G


create a rdd through a collection
---------------------------------
val no = Array(1,2,3,4,5)
val myData = sc.parallelize(no,2) //parallelize will create a RDD on collection
myData.partitions.length   ----number of partitions  = number of tasks
myData.count()


2. Convert text to lower case
------------------------------
val inputfile = sc.textFile("/home/hduser/sample.txt"); -- Base RDD

val inputfile1 = sc.textFile("hdfs://localhost:54310/user/hduser/file1");

val lower = inputfile.map(a => a.toLowerCase());

val upper = inputfile.map(a => a.toUpperCase());

lower.foreach(println);

inputfile.foreach(println);

3. Convert text to upper case
------------------------------
val inputfile = sc.textFile("/home/hduser/sample.txt");

val upper = inputfile.map(a => a.toUpperCase());


upper.foreach(println);

4. Calc the length of a file
-----------------------------
val lines = sc.textFile("/home/hduser/sample.txt");

val linelength = inputfile.map(s => s.length);


OR

val linelength = lines.map(_.length);

linelength.foreach(println);

val totallength = linelength.reduce((a,b) => a+b);




val totallength = linelength.reduce(_+_);

println("Total Length for all characters : " + totallength);



5. Word Count using spark
-------------------------
val inputfile = sc.textFile("hdfs://localhost:54310/user/hduser/imarticus/file1");

inputfile.foreach(println);

val transform = inputfile.map(line => line.split(" "));
transform.collect()


val transform = inputfile.flatMap(line => line.split(" "));

OR

val transform = inputfile.flatMap(_.split(" "));

transform.foreach(println);

val keybyword = transform.map(word => (word, 1));

keybyword.foreach(println);

val counts = keybyword.reduceByKey(_+_).sortByKey();

OR

val counts = keybyword.reduceByKey((a,b) => a+b).sortByKey();

//asc
val sortbyval = counts.collect.sortBy(_._2);

//desc
val sortbyval = counts.collect.sortBy(-_._2);


sortbyval.foreach(println);

counts.foreach(println);

//storing an RDD in an output file

counts.saveAsTextFile("hdfs://localhost:54310/user/hduser/imarticus/spark1");

counts.saveAsTextFile("/home/hduser/spark1");



//storing an array in an output file

sc.parallelize(sortbyval).saveAsTextFile("hdfs://localhost:54310/user/hduser/imarticus/spark2");




skip first line in spark
------------------------
val lines = sc.textFile("/home/hduser/airlines.csv")
val header = lines.first()
pppval newlines = lines.filter(a => a != header)


6.log file
---------
val lines = sc.textFile("/home/hduser/mapred-hduser-historyserver-ubuntu.log")
val lines_lower = lines.map(a => a.toLowerCase());
val errors = lines_lower.filter(a => a.contains("error"))
errors.cache()
errors.filter(a => a.contains("sigterm")).count()
errors.filter(a => a.contains("jobhistoryserver")).count()


6. Returns the count of records
-------------------------------
val custRDD = sc.textFile("/home/hduser/custs.txt");
custRDD.count();

val txnRDD = sc.textFile("/home/hduser/txns1.txt");
txnRDD.count();

txnRDD.partitions.size


7.Mapping customer table to Profession and record => SQL counting the number of customers per profession;(sort on value)
and find top ten professions on count.
------------------------------------------------
val custRDD = sc.textFile("/home/hduser/custs.txt",1)

val professionRDD = custRDD.map(x => (x.split(",")(4), 1))
professionRDD.take(10).foreach(println)


val professioncounts = professionRDD.reduceByKey(_+_).sortByKey(false);//sort the key in desc else true for asc
professioncounts.foreach(println);

val sortbyval = professioncounts.collect.sortBy(-_._2);
val top10prof = sortbyval.take(10);
sc.parallelize(sortbyval).saveAsTextFile("/home/hduser/profession_count");



8.1 calculating total amount spent by each customer and find top ten buyers
------------------------------------------------------------------------
val txnRDD = sc.textFile("/home/hduser/txns1.txt",1)

val custRDD = txnRDD.map(x => (x.split(",")(2), x.split(",")(3).toDouble ))
custRDD.take(10).foreach(println);

val custTotalSpent = custRDD.reduceByKey((a,b) => a+b);
custTotalSpent.foreach(println);
val sortbyval = custTotalSpent.collect.sortBy(-_._2);
val top10 = sortbyval.take(10);
top10.foreach(println);

val top10RDD =sc.parallelize(top10)

val custRDD1 = sc.textFile("/home/hduser/custs.txt")
val custRDD11 = custRDD1.map(x => (x.split(",")(0), x.split(",")(1)))

val joined = top10RDD.leftOuterJoin(custRDD11).map { case (a, (b, c: Option[String])) => (a, (b, (c.getOrElse("unknown")))) }

joined.foreach(println);

//removing round bracket
val transform = joined.map{case (a,b) => (a, (b.x._1+","+b.x._2))};

val transform2 = transform.map( x => (x._1 + "," + x._2))


val transform3 = transform2.map(x => (x.split(",")(0) + "," + x.split(",")(2), x.split(",")(1).toDouble ))

val sortbyval2 = transform3.collect.sortBy(-_._2);

sortbyval2.foreach(println)

sc.parallelize(sortbyval2).saveAsTextFile("/home/hduser/spark1/topten");


8.2 find the top ten products amount wise
-----------------------------------------
val txnRDD = sc.textFile("/home/hduser/txns1.txt")
val ProdRDD = txnRDD.map(x => (x.split(",")(5), x.split(",")(3).toDouble ))
val ProdTotalSpent = ProdRDD.reduceByKey((a,b) => a+b).sortByKey()
val sortbyval = ProdTotalSpent.collect.sortBy(-_._2)
sortbyval.take(10).foreach(println)


9.Find out the customer I.D for the customer who has spent the maximum amount in a single transaction. 
------------------------------------------------------------------------------------------------------
val retailRDD = sc.textFile("/home/hduser/Retail_Data")

//val retailRDD = sc.textFile("hdfs://localhost:54310/retail")

retailRDD.count()

val retail = retailRDD.map(line => line.split(";").map(_.trim))

retail.collect()

val reqDet = retail.map(x=>x(8).toInt)

reqDet.foreach(println);

val maxAmount = reqDet.max

val cusDet = retail.filter(x=>x(8).toInt == maxAmount)

cusDet.collect

cusDet.map(x=>x(1)).collect


9.1 Find out the top 10 viable products 
----------------------------------------
val retailRDD = sc.textFile("/home/hduser/Retail_Data")

val retail = retailRDD.map(line => line.split(";").map(_.trim))

val reqDet = retail.map(x=> (x(5), (x(8).toInt - x(7).toInt)))

val profitforproduct = reqDet.reduceByKey((a,b) => a+b)

profitforproduct.foreach(println)

val sortbyval = profitforproduct.collect.sortBy(-_._2)

OR

val sortbyval = profitforproduct.collect.sortBy(x => -x._2)

val top10 = sortbyval.take(10);

top10.foreach(println)


9.2 find the age wise sales
---------------------------
val retailRDD = sc.textFile("/home/hduser/Retail_Data")

val retail = retailRDD.map(line => line.split(";").map(_.trim))

val ageRDD = retail.map(x=> (x(2), x(8).toInt))

val agewiseSales = ageRDD.reduceByKey((a,b) => a+b)

val sortbyval = agewiseSales.collect.sortBy(-_._2)

sortbyval.foreach(println)



9.3 find out the net margin% for each product
----------------------------------------
val retailRDD = sc.textFile("/home/hduser/Retail_Data")

val retail = retailRDD.map(line => line.split(";").map(_.trim))

val sales = retail.map(x=> (x(5),(x(8).toInt)))

val totalsales = sales.reduceByKey((a,b) => a+b).sortByKey();

totalsales.foreach(println);

val purchase = retail.map(x=> (x(5),(x(7).toInt)))

val totalpurchase = purchase.reduceByKey((a,b) => a+b).sortByKey();

totalpurchase.foreach(println);

val joined = totalsales.fullOuterJoin(totalpurchase).map { case (a, (b, c: Option[Int])) => (a, (b.getOrElse().asInstanceOf[Int], (c.getOrElse().asInstanceOf[Int]))) }

val net = joined.map(pair => (pair._1, (pair._2._1 - pair._2._2)))



10. Partitioners
--------------------
import org.apache.spark.HashPartitioner
val inputfile = sc.textFile("sample.txt");
val transform = inputfile.flatMap(line => line.split(" "));
val keybyword = transform.map(word => (word, 1));
val myPartition = keybyword.partitionBy(new HashPartitioner(2))
val counts = myPartition.reduceByKey((a,b) => a+b).sortByKey();
counts.saveAsTextFile("/home/hduser/folder1/part2");

counts.partitions.size 
Int = 2

import org.apache.spark.RangePartitioner
val inputfile = sc.textFile("sample.txt");
val transform = inputfile.flatMap(line => line.split(" "));
val keybyword = transform.map(word => (word, 1));
val myPartition = keybyword.partitionBy(new RangePartitioner(3,keybyword))
val counts = myPartition.reduceByKey((a,b) => a+b).sortByKey();
counts.saveAsTextFile("/home/hduser/folder2/part4");




